{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameter Tuning - Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nohayjupytersingif](https://media.giphy.com/media/jeDM590qtCP9C/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Peque침a-exploraci칩n-de-los-datos\" data-toc-modified-id=\"Peque침a-exploraci칩n-de-los-datos-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Peque침a exploraci칩n de los datos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nos-fijamos-en-la-feature-&quot;Cabin&quot;\" data-toc-modified-id=\"Nos-fijamos-en-la-feature-&quot;Cabin&quot;-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Nos fijamos en la feature \"Cabin\"</a></span></li><li><span><a href=\"#Analizamos-los-nombres-de-los-pasajeros\" data-toc-modified-id=\"Analizamos-los-nombres-de-los-pasajeros-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Analizamos los nombres de los pasajeros</a></span></li></ul></li><li><span><a href=\"#Categorical-encoding\" data-toc-modified-id=\"Categorical-encoding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Categorical encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoder\" data-toc-modified-id=\"One-Hot-Encoder-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One Hot Encoder</a></span></li><li><span><a href=\"#Label-Encoder\" data-toc-modified-id=\"Label-Encoder-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Label Encoder</a></span></li><li><span><a href=\"#A-mano-con-un-diccionario-游눠\" data-toc-modified-id=\"A-mano-con-un-diccionario-游눠-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>A mano con un diccionario 游눠</a></span></li></ul></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estandarizaci칩n\" data-toc-modified-id=\"Estandarizaci칩n-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Estandarizaci칩n</a></span></li><li><span><a href=\"#Normalizaci칩n-min-max\" data-toc-modified-id=\"Normalizaci칩n-min-max-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Normalizaci칩n min-max</a></span></li><li><span><a href=\"#Cito-a-Andriy-Burkov:\" data-toc-modified-id=\"Cito-a-Andriy-Burkov:-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Cito a Andriy Burkov:</a></span></li></ul></li><li><span><a href=\"#Repasamos-Train-Test-Split\" data-toc-modified-id=\"Repasamos-Train-Test-Split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Repasamos Train Test Split</a></span></li><li><span><a href=\"#Ajuste-de-hiperpar치metros\" data-toc-modified-id=\"Ajuste-de-hiperpar치metros-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ajuste de hiperpar치metros</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Muestreo-aleatorio\" data-toc-modified-id=\"--Muestreo-aleatorio-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>- Muestreo aleatorio</a></span></li><li><span><a href=\"#--Muestreo-de-cuadr칤cula\" data-toc-modified-id=\"--Muestreo-de-cuadr칤cula-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>- Muestreo de cuadr칤cula</a></span></li><li><span><a href=\"#--Muestreo-bayesiano\" data-toc-modified-id=\"--Muestreo-bayesiano-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>- Muestreo bayesiano</a></span></li><li><span><a href=\"#GridSearchCV-de-sklearn,-춰saludad-a-vuestro-nuevo-amigo!\" data-toc-modified-id=\"GridSearchCV-de-sklearn,-춰saludad-a-vuestro-nuevo-amigo!-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>GridSearchCV de sklearn, 춰saludad a vuestro nuevo amigo!</a></span></li><li><span><a href=\"#Entrenar칤amos-el-modelo-con-los-mejores-par치metros\" data-toc-modified-id=\"Entrenar칤amos-el-modelo-con-los-mejores-par치metros-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Entrenar칤amos el modelo con los mejores par치metros</a></span></li></ul></li><li><span><a href=\"#Salvar-/-Exprotar-el-modelo\" data-toc-modified-id=\"Salvar-/-Exprotar-el-modelo-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Salvar / Exprotar el modelo</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el proceso de utilizar el conocimiento del dominio para extraer caracter칤sticas de los datos brutos.  \n",
    "Estas caracter칤sticas pueden utilizarse para mejorar el rendimiento de los algoritmos de aprendizaje autom치tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/titanic.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Peque침a exploraci칩n de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el porcentaje de nulos en cada columna\n",
    "round(data.isnull().sum().sort_values(ascending=False)/len(data)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos fijamos en la feature \"Cabin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos valores que faltan, pero debemos utilizar la variable del camarote porque puede ser un predictor importante. Como se puede ver en la siguiente imagen, la primera clase ten칤a los camarotes en la cubierta A, B o C, una mezcla estaba en la D o la E y la tercera clase estaba principalmente en la f o la g. Podemos identificar la cubierta por la primera letra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![laimagendelbarco](../images/barco.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna \"Cubierta\" bas치ndonos en la letra del camarote\n",
    "data[\"Deck\"] = data[\"Cabin\"].apply(lambda x: x[0] if pd.notnull(x) else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ㅁnalizamos los nombres de los pasajeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nombre podr칤aa aportarnos informaci칩n importante sobre el estatus socioecon칩mico de un pasajero. Y en funci칩n del estatus socioecon칩mico han podido comprar un billete m치s caro o m치s barato, que indica un camarote situado en uno u otro lugar del barco. Podemos responder a la pregunta de si alguien est치 casado o no o si tiene un t칤tulo formal y extraer esa informaci칩n para generar una nueva variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpianame(x):\n",
    "    x = x.split(\",\")\n",
    "    x = x[1].split(\".\")\n",
    "    return x[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Title\"] = data[\"Name\"].apply(limpianame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para no tener muchas categor칤as con los t칤tulos, vamos a quedarnos con los que tienen m치s de 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varios = (data.Title.value_counts() < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un loc de la columna y si es true, porque es menos de 10, lo identificamos como miscel치neo\n",
    "data[\"Title\"] = data[\"Title\"].apply(lambda x: \"Misc\" if varios.loc[x] == True else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hemos agrupado los t칤tulos con menos de 10 registros en una categor칤a nueva\n",
    "data.Title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos las columnas que vamos a querer despreciar porque ya hemos trabajado con ellas o no nos aportan informaci칩n.\n",
    "borrar = [\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(borrar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos queda la columna Age con nulos ... Vamos a rellenarlos, pero explorando los datos.... 쯦ienen la misma edad de media los hombres que las mujeres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos por sexo y edad para ver las medianas de ambas agrupaciones\n",
    "display(data.groupby([\"Sex\"])[\"Age\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos adem치s por cubierta para tener en cuenta tambi칠n el estatus socioecon칩mico\n",
    "display(data.groupby([\"Sex\", \"Deck\"])[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ajustarnos un poco m치s, vamos a rellenar los NaN de la edad con la mediana pero en funci칩n de su sexo y tambi칠n en funci칩n de la cubierta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Age\"] = data.groupby([\"Sex\", \"Deck\"])[\"Age\"].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar columnas categ칩ricas en num칠ricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, como ya hemos comentado, dependiendo de los datos que tengamos, podr칤amos encontrarnos con situaciones en las que, tras la codificaci칩n de las etiquetas, podr칤amos confundir a nuestro modelo haci칠ndole creer que una columna tiene datos con alg칰n tipo de orden o jerarqu칤a, cuando claramente no lo tenemos. Para evitar esto, \"OneHotEncode\" esa columna.\n",
    "Lo que hace una codificaci칩n en caliente es que toma una columna que tiene datos categ칩ricos, que ha sido codificada con etiquetas, y luego divide la columna en m칰ltiples columnas. Los n칰meros son reemplazados por 1s y 0s, dependiendo de qu칠 columna tiene qu칠 valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacer lo mismo con los t칤tulos pero con OneHotEncoder, que nos crear치 diferentes columnas\n",
    "data.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista con lo que van a ser los nombres de las columnas\n",
    "labels = [\"Title_\" + str(a) for a in list(data.Title.unique())]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el fit transform y nos guardamos los array de datos en una variable\n",
    "title = onehotencoder.fit_transform(data[\"Title\"].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A침adimos al dataframe todas las columnas a la vez\n",
    "data[labels] = pd.DataFrame(title,index = data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La columna Title ya no nos hace falta.\n",
    "data.drop(\"Title\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna nueva que es \"embarked_n\" con la transformaci칩n a num칠rica de las categor칤as \n",
    "data[\"Embarked_n\"] = le.fit_transform(data[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos lo mismo con sex, a trav칠s de LabelEncoder\n",
    "data[\"Sex_n\"] = le.fit_transform(data[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recordemos que labelencoder nos pone n칰meros donde tenemos categor칤as empezando por 0 e incrementando\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Embarked\", \"Sex\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ㅁ mano con un diccionario 游눠\n",
    "Podremos otorgarle un valor num칠rico a cada categor칤a y decidimos su importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Deck.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creamos las categor칤as a mano, podr칤amos darle un orden de importancia a las letras y poner\n",
    "mayor puntuaci칩n o menor en funci칩n de la relaci칩n que tenga esa variable con la variable target.\n",
    "\"\"\"\n",
    "dic_para_hot = { \"M\": 1,\n",
    "                \"C\": 2,\n",
    "                \"E\": 3,\n",
    "                \"G\":4,\n",
    "                \"D\":5,\n",
    "                \"A\":6,\n",
    "                \"B\":7,\n",
    "                \"F\": 8,\n",
    "                \"T\":9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con un map reemplazamos todas las strings de la columna Deck por el valor asignado en el diccionario\n",
    "data.Deck = data.Deck.map(dic_para_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos algoritmos, especialmente los que se basan en c치lculos de distancia, dar치n m치s peso a las caracter칤sticas que muestren grandes cambios de valor, interpretando estas caracter칤sticas como artificialmente m치s importantes. Para estos algoritmos, es importante que escalemos nuestros rasgos, o que pongamos en la misma escala rasgos con escalas naturalmente diferentes, para que los rasgos sean utilizados por el algoritmo sin una sobreponderaci칩n artificial, y permita comparar dos rasgos con escalas diferentes.      \n",
    "Hay dos tipos diferentes de escalamiento de caracter칤sticas que vamos a explorar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarizaci칩n   \n",
    "En la estandarizaci칩n, imponemos varias propiedades estad칤sticas a la variable: el valor medio se fija en 0, y la desviaci칩n est치ndar se fija en 1. Esto se consigue restando la media de cada valor de la caracter칤stica y dividiendo por la desviaci칩n est치ndar. Esto tambi칠n se llama a veces \"normalizaci칩n de la puntuaci칩n z\". \n",
    "\n",
    "Entonces, 쯤u칠 significa esto, en la pr치ctica, sobre los datos estandarizados? Como podemos ver a continuaci칩n, ahora tenemos las distribuciones de ambas variables centradas alrededor de la media cero, con una desviaci칩n est치ndar de 1. Como estamos imponiendo esta desviaci칩n est치ndar, la normalizaci칩n reduce los efectos de los valores at칤picos en la caracter칤stica. Adem치s, permite comparar dos caracter칤sticas con escalas o unidades diferentes. Las diferentes escalas de las caracter칤sticas se reflejar칤an estad칤sticamente en diferencias tanto en la media como en la desviaci칩n est치ndar. La estandarizaci칩n de estos dos n칰meros entre caracter칤sticas elimina la influencia de estas diferencias de escala.\n",
    "\n",
    "La estandarizaci칩n es especialmente importante en situaciones en las que utilizamos algoritmos que asumen que las caracter칤sticas de nuestros datos se distribuyen en una 'curva de campana' o una distribuci칩n gaussiana, como la regresi칩n lineal y log칤stica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "En este dataset no nos enfrentamos a un problema de regresi칩n si no de clasificaci칩n,\n",
    "pero vamos a hacer un ejemplo de estandarizaci칩n en una columna para ver el c칩digo\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Fare\"] = scaler.fit_transform(data[\"Fare\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizaci칩n min-max\n",
    "\n",
    "En la otra forma de escalado de caracter칤sticas, llamada normalizaci칩n, la caracter칤stica se reescala a un rango entre 0 y 1, sin ning칰n cambio en su distribuci칩n original dentro de ese rango. Matem치ticamente, esto se consigue restando el valor m칤nimo de la caracter칤stica a cada valor de la misma, y dividiendo por la diferencia entre el valor mayor y el valor m칤nimo. \n",
    "\n",
    "Dado que calculamos el valor normalizado utilizando los valores m치ximo y m칤nimo de la caracter칤stica, esta t칠cnica se denomina a veces \"normalizaci칩n min-max\".      \n",
    "La normalizaci칩n es m치s 칰til en los casos en que sus datos tienen pocos valores at칤picos pero rangos muy variables, usted no sabe c칩mo se distribuyen sus datos, o sabe que no se distribuyen en una curva de campana (gaussiana). Generalmente se aplica con algoritmos que no hacen suposiciones sobre las distribuciones de las caracter칤sticas.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Age\"] = min_max.fit_transform(data[\"Age\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cito a Andriy Burkov:\n",
    "Te estar치s preguntando cu치ndo se debe utilizar la normalizaci칩n y cu치ndo la estandarizaci칩n. No hay una respuesta definitiva a esta pregunta. Por lo general, si su conjunto de datos no es demasiado grande y tiene tiempo, puede probar ambos y ver cu치l de ellos se adapta mejor a su tarea.\n",
    "Si no tiene tiempo para realizar varios experimentos, como regla general:\n",
    "\n",
    "- Los algoritmos de aprendizaje no supervisado, en la pr치ctica, se benefician m치s de la estandarizaci칩n que de la normalizaci칩n.      \n",
    "- La estandarizaci칩n tambi칠n es preferible para una caracter칤stica si los valores que 칠sta toma se distribuyen cerca de una distribuci칩n normal (la llamada curva de campana).     \n",
    "- Una vez m치s, la normalizaci칩n es preferible para una caracter칤stica si a veces puede tener valores extremadamente altos o bajos (valores at칤picos); esto se debe a que la normalizaci칩n \"exprimir치\" los valores normales en un rango muy peque침o.       \n",
    "- En todos los dem치s casos, es preferible la normalizaci칩n min-max.      \n",
    "\n",
    "El reescalado de caracter칤sticas suele ser beneficioso para la mayor칤a de los algoritmos de aprendizaje. Sin embargo, las implementaciones modernas de los algoritmos de aprendizaje, que se pueden encontrar en bibliotecas populares, son robustas a las caracter칤sticas que se encuentran en diferentes rangos.\n",
    "\n",
    "**NOTA**: a los modelos de tipo 치rbol (DecisionTree, RandomForest, GradientBoosting) les da igual la normalizaci칩n. No les importa la magnitud exacta de una variable, si no la ordenaci칩n de los valores (s칩lo hacen preguntas < o > qu칠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Repasamos Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a preparar los datos (X, y) antes de entrenar el modelo y ajustar los hiperpar치metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas que voy a usar en la X\n",
    "columnas_x = [a for a in list(data.columns) if a != \"Survived\"]\n",
    "columnas_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me guardo la variable X con todos los datos \n",
    "X = data[columnas_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable target solo la columna que voy a predecir\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asigno las variables train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de hiperpar치metros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쯈u칠 es el ajuste de hiperpar치metros?\n",
    "Los hiperpar치metros son par치metros ajustables que permiten controlar el proceso de entrenamiento de un modelo. Por ejemplo, con redes neuronales, puede decidir el n칰mero de capas ocultas y el n칰mero de nodos de cada capa. El rendimiento de un modelo depende en gran medida de los hiperpar치metros.\n",
    "El ajuste de hiperpar치metros, tambi칠n denominado optimizaci칩n de hiperpar치metros es el proceso de encontrar la configuraci칩n de hiperpar치metros que produzca el mejor rendimiento. Normalmente, el proceso es manual y costoso desde el punto de vista computacional.\n",
    "\n",
    "Hay diferentes t칠cnicas para elegir este ajuste de hiperpar치metros:     \n",
    "    \n",
    "### - Muestreo aleatorio    \n",
    "El muestreo aleatorio admite hiperpar치metros discretos y continuos. Admite la terminaci칩n anticipada de las series de bajo rendimiento. Algunos usuarios realizan una b칰squeda inicial con muestreo aleatorio y luego restringen el espacio de b칰squeda para mejorar los resultados.\n",
    "En el muestreo aleatorio, los valores de hiperpar치metro se seleccionan aleatoriamente del espacio de b칰squeda definido.\n",
    "\n",
    "### - Muestreo de cuadr칤cula\n",
    "El muestreo de cuadr칤cula admite hiperpar치metros discretos. Use el muestreo de cuadr칤cula si su presupuesto le permite buscar en el espacio de b칰squeda de manera exhaustiva. Admite la terminaci칩n anticipada de las series de bajo rendimiento.\n",
    "\n",
    "### - Muestreo bayesiano   \n",
    "El muestreo bayesiano se basa en el algoritmo de optimizaci칩n bayesiano. Escoge las muestras en funci칩n de c칩mo lo hicieron las anteriores, para que las nuevas muestras mejoren la m칠trica principal.\n",
    " Para obtener los mejores resultados, se recomienda que el n칰mero m치ximo de series sea mayor o igual que 20 veces el n칰mero de hiperpar치metros que se est치 optimizando.\n",
    "El n칰mero de series simult치neas afecta a la eficacia del proceso de ajuste. Un menor n칰mero de series simult치neas puede provocar una mejor convergencia de muestreo, dado que el menor grado de paralelismo aumenta el n칰mero de series que se benefician de las series completadas previamente.\n",
    "\n",
    "Vamos a ver el ajuste de hiperpar치metros en cuadr칤cula con GridSearchCV pero os dejo que investigu칠is el muestreo bayesiano con [HyperOpt](https://towardsdatascience.com/hyperopt-hyperparameter-tuning-based-on-bayesian-optimization-7fa32dffaf29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###GridSearchCV de sklearn, 춰saludad a vuestro nuevo amigo!\n",
    "Y leed la [documentaci칩n](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperpar치metros tuneables de RandomForest\n",
    "parameters = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducimos para hacer la prueba con diferentes n_estimators\n",
    "params = {\n",
    "     'n_estimators': [400, 600,800]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaardo en una variable raandom forest\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo el grid search con el algoritmo, los par치metros y verbose paraa que muestre info del proceso\n",
    "grid = GridSearchCV(rfc, params, verbose=1)\n",
    "# entreno el grid con los datos de train\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo los mejores par치metros que me ha dado el modelo\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Entrenar칤amos el modelo con los mejores par치metros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si en GridSearchCV() se indica refit=True, tras identificar los mejores hiperpar치metros, se reentrena el modelo con ellos y se almacena en .best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenon\n",
    "rfc_params = RandomForestClassifier(n_estimators =  800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy\", round(accuracy_score(y_test,y_pred),3))\n",
    "print(\"Precission\",round(precision_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"Recall\", round(recall_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"F1_score\", round(f1_score(y_test,y_pred,average= \"weighted\"),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Salvar / Exprotar el modelo\n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#맙ave the model to disk\n",
    "pickle.dump(rfc_params, open(\"mi_mejor_modelo\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(\"mi_mejor_modelo\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Hyperparameter Tuning - Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nohayjupytersingif](https://media.giphy.com/media/jeDM590qtCP9C/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Feature engineering</a></span></li><li><span><a href=\"#Pequeña-exploración-de-los-datos\" data-toc-modified-id=\"Pequeña-exploración-de-los-datos-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pequeña exploración de los datos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nos-fijamos-en-la-feature-&quot;Cabin&quot;\" data-toc-modified-id=\"Nos-fijamos-en-la-feature-&quot;Cabin&quot;-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Nos fijamos en la feature \"Cabin\"</a></span></li><li><span><a href=\"#Analizamos-los-nombres-de-los-pasajeros\" data-toc-modified-id=\"Analizamos-los-nombres-de-los-pasajeros-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Analizamos los nombres de los pasajeros</a></span></li></ul></li><li><span><a href=\"#Categorical-encoding\" data-toc-modified-id=\"Categorical-encoding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Categorical encoding</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot-Encoder\" data-toc-modified-id=\"One-Hot-Encoder-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One Hot Encoder</a></span></li><li><span><a href=\"#Label-Encoder\" data-toc-modified-id=\"Label-Encoder-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Label Encoder</a></span></li><li><span><a href=\"#A-mano-con-un-diccionario-💡\" data-toc-modified-id=\"A-mano-con-un-diccionario-💡-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>A mano con un diccionario 💡</a></span></li></ul></li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Scaling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estandarización\" data-toc-modified-id=\"Estandarización-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Estandarización</a></span></li><li><span><a href=\"#Normalización-min-max\" data-toc-modified-id=\"Normalización-min-max-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Normalización min-max</a></span></li><li><span><a href=\"#Cito-a-Andriy-Burkov:\" data-toc-modified-id=\"Cito-a-Andriy-Burkov:-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Cito a Andriy Burkov:</a></span></li></ul></li><li><span><a href=\"#Repasamos-Train-Test-Split\" data-toc-modified-id=\"Repasamos-Train-Test-Split-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Repasamos Train Test Split</a></span></li><li><span><a href=\"#Ajuste-de-hiperparámetros\" data-toc-modified-id=\"Ajuste-de-hiperparámetros-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ajuste de hiperparámetros</a></span><ul class=\"toc-item\"><li><span><a href=\"#--Muestreo-aleatorio\" data-toc-modified-id=\"--Muestreo-aleatorio-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>- Muestreo aleatorio</a></span></li><li><span><a href=\"#--Muestreo-de-cuadrícula\" data-toc-modified-id=\"--Muestreo-de-cuadrícula-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>- Muestreo de cuadrícula</a></span></li><li><span><a href=\"#--Muestreo-bayesiano\" data-toc-modified-id=\"--Muestreo-bayesiano-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>- Muestreo bayesiano</a></span></li><li><span><a href=\"#GridSearchCV-de-sklearn,-¡saludad-a-vuestro-nuevo-amigo!\" data-toc-modified-id=\"GridSearchCV-de-sklearn,-¡saludad-a-vuestro-nuevo-amigo!-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>GridSearchCV de sklearn, ¡saludad a vuestro nuevo amigo!</a></span></li><li><span><a href=\"#Entrenaríamos-el-modelo-con-los-mejores-parámetros\" data-toc-modified-id=\"Entrenaríamos-el-modelo-con-los-mejores-parámetros-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Entrenaríamos el modelo con los mejores parámetros</a></span></li></ul></li><li><span><a href=\"#Salvar-/-Exprotar-el-modelo\" data-toc-modified-id=\"Salvar-/-Exprotar-el-modelo-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Salvar / Exprotar el modelo</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el proceso de utilizar el conocimiento del dominio para extraer características de los datos brutos.  \n",
    "Estas características pueden utilizarse para mejorar el rendimiento de los algoritmos de aprendizaje automático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/titanic.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pequeña exploración de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos el porcentaje de nulos en cada columna\n",
    "round(data.isnull().sum().sort_values(ascending=False)/len(data)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos fijamos en la feature \"Cabin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Cabin.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchos valores que faltan, pero debemos utilizar la variable del camarote porque puede ser un predictor importante. Como se puede ver en la siguiente imagen, la primera clase tenía los camarotes en la cubierta A, B o C, una mezcla estaba en la D o la E y la tercera clase estaba principalmente en la f o la g. Podemos identificar la cubierta por la primera letra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![laimagendelbarco](../images/barco.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna \"Cubierta\" basándonos en la letra del camarote\n",
    "data[\"Deck\"] = data[\"Cabin\"].apply(lambda x: x[0] if pd.notnull(x) else \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizamos los nombres de los pasajeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nombre podríaa aportarnos información importante sobre el estatus socioeconómico de un pasajero. Y en función del estatus socioeconómico han podido comprar un billete más caro o más barato, que indica un camarote situado en uno u otro lugar del barco. Podemos responder a la pregunta de si alguien está casado o no o si tiene un título formal y extraer esa información para generar una nueva variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpianame(x):\n",
    "    x = x.split(\",\")\n",
    "    x = x[1].split(\".\")\n",
    "    return x[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Title\"] = data[\"Name\"].apply(limpianame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para no tener muchas categorías con los títulos, vamos a quedarnos con los que tienen más de 10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varios = (data.Title.value_counts() < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un loc de la columna y si es true, porque es menos de 10, lo identificamos como misceláneo\n",
    "data[\"Title\"] = data[\"Title\"].apply(lambda x: \"Misc\" if varios.loc[x] == True else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemos agrupado los títulos con menos de 10 registros en una categoría nueva\n",
    "data.Title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos las columnas que vamos a querer despreciar porque ya hemos trabajado con ellas o no nos aportan información.\n",
    "borrar = [\"Name\", \"PassengerId\", \"Cabin\", \"Ticket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(borrar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo nos queda la columna Age con nulos ... Vamos a rellenarlos, pero explorando los datos.... ¿tienen la misma edad de media los hombres que las mujeres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos por sexo y edad para ver las medianas de ambas agrupaciones\n",
    "display(data.groupby([\"Sex\"])[\"Age\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos además por cubierta para tener en cuenta también el estatus socioeconómico\n",
    "display(data.groupby([\"Sex\", \"Deck\"])[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ajustarnos un poco más, vamos a rellenar los NaN de la edad con la mediana pero en función de su sexo y también en función de la cubierta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Age\"] = data.groupby([\"Sex\", \"Deck\"])[\"Age\"].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformar columnas categóricas en numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, como ya hemos comentado, dependiendo de los datos que tengamos, podríamos encontrarnos con situaciones en las que, tras la codificación de las etiquetas, podríamos confundir a nuestro modelo haciéndole creer que una columna tiene datos con algún tipo de orden o jerarquía, cuando claramente no lo tenemos. Para evitar esto, \"OneHotEncode\" esa columna.\n",
    "Lo que hace una codificación en caliente es que toma una columna que tiene datos categóricos, que ha sido codificada con etiquetas, y luego divide la columna en múltiples columnas. Los números son reemplazados por 1s y 0s, dependiendo de qué columna tiene qué valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacer lo mismo con los títulos pero con OneHotEncoder, que nos creará diferentes columnas\n",
    "data.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una lista con lo que van a ser los nombres de las columnas\n",
    "labels = [\"Title_\" + str(a) for a in list(data.Title.unique())]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos el fit transform y nos guardamos los array de datos en una variable\n",
    "title = onehotencoder.fit_transform(data[\"Title\"].values.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añadimos al dataframe todas las columnas a la vez\n",
    "data[labels] = pd.DataFrame(title,index = data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La columna Title ya no nos hace falta.\n",
    "data.drop(\"Title\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna nueva que es \"embarked_n\" con la transformación a numérica de las categorías \n",
    "data[\"Embarked_n\"] = le.fit_transform(data[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos lo mismo con sex, a través de LabelEncoder\n",
    "data[\"Sex_n\"] = le.fit_transform(data[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recordemos que labelencoder nos pone números donde tenemos categorías empezando por 0 e incrementando\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"Embarked\", \"Sex\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A mano con un diccionario 💡\n",
    "Podremos otorgarle un valor numérico a cada categoría y decidimos su importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Deck.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creamos las categorías a mano, podríamos darle un orden de importancia a las letras y poner\n",
    "mayor puntuación o menor en función de la relación que tenga esa variable con la variable target.\n",
    "\"\"\"\n",
    "dic_para_hot = { \"M\": 1,\n",
    "                \"C\": 2,\n",
    "                \"E\": 3,\n",
    "                \"G\":4,\n",
    "                \"D\":5,\n",
    "                \"A\":6,\n",
    "                \"B\":7,\n",
    "                \"F\": 8,\n",
    "                \"T\":9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con un map reemplazamos todas las strings de la columna Deck por el valor asignado en el diccionario\n",
    "data.Deck = data.Deck.map(dic_para_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos algoritmos, especialmente los que se basan en cálculos de distancia, darán más peso a las características que muestren grandes cambios de valor, interpretando estas características como artificialmente más importantes. Para estos algoritmos, es importante que escalemos nuestros rasgos, o que pongamos en la misma escala rasgos con escalas naturalmente diferentes, para que los rasgos sean utilizados por el algoritmo sin una sobreponderación artificial, y permita comparar dos rasgos con escalas diferentes.      \n",
    "Hay dos tipos diferentes de escalamiento de características que vamos a explorar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización   \n",
    "En la estandarización, imponemos varias propiedades estadísticas a la variable: el valor medio se fija en 0, y la desviación estándar se fija en 1. Esto se consigue restando la media de cada valor de la característica y dividiendo por la desviación estándar. Esto también se llama a veces \"normalización de la puntuación z\". \n",
    "\n",
    "Entonces, ¿qué significa esto, en la práctica, sobre los datos estandarizados? Como podemos ver a continuación, ahora tenemos las distribuciones de ambas variables centradas alrededor de la media cero, con una desviación estándar de 1. Como estamos imponiendo esta desviación estándar, la normalización reduce los efectos de los valores atípicos en la característica. Además, permite comparar dos características con escalas o unidades diferentes. Las diferentes escalas de las características se reflejarían estadísticamente en diferencias tanto en la media como en la desviación estándar. La estandarización de estos dos números entre características elimina la influencia de estas diferencias de escala.\n",
    "\n",
    "La estandarización es especialmente importante en situaciones en las que utilizamos algoritmos que asumen que las características de nuestros datos se distribuyen en una 'curva de campana' o una distribución gaussiana, como la regresión lineal y logística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "En este dataset no nos enfrentamos a un problema de regresión si no de clasificación,\n",
    "pero vamos a hacer un ejemplo de estandarización en una columna para ver el código\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Fare\"] = scaler.fit_transform(data[\"Fare\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización min-max\n",
    "\n",
    "En la otra forma de escalado de características, llamada normalización, la característica se reescala a un rango entre 0 y 1, sin ningún cambio en su distribución original dentro de ese rango. Matemáticamente, esto se consigue restando el valor mínimo de la característica a cada valor de la misma, y dividiendo por la diferencia entre el valor mayor y el valor mínimo. \n",
    "\n",
    "Dado que calculamos el valor normalizado utilizando los valores máximo y mínimo de la característica, esta técnica se denomina a veces \"normalización min-max\".      \n",
    "La normalización es más útil en los casos en que sus datos tienen pocos valores atípicos pero rangos muy variables, usted no sabe cómo se distribuyen sus datos, o sabe que no se distribuyen en una curva de campana (gaussiana). Generalmente se aplica con algoritmos que no hacen suposiciones sobre las distribuciones de las características.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Age\"] = min_max.fit_transform(data[\"Age\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Age.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cito a Andriy Burkov:\n",
    "Te estarás preguntando cuándo se debe utilizar la normalización y cuándo la estandarización. No hay una respuesta definitiva a esta pregunta. Por lo general, si su conjunto de datos no es demasiado grande y tiene tiempo, puede probar ambos y ver cuál de ellos se adapta mejor a su tarea.\n",
    "Si no tiene tiempo para realizar varios experimentos, como regla general:\n",
    "\n",
    "- Los algoritmos de aprendizaje no supervisado, en la práctica, se benefician más de la estandarización que de la normalización.      \n",
    "- La estandarización también es preferible para una característica si los valores que ésta toma se distribuyen cerca de una distribución normal (la llamada curva de campana).     \n",
    "- Una vez más, la normalización es preferible para una característica si a veces puede tener valores extremadamente altos o bajos (valores atípicos); esto se debe a que la normalización \"exprimirá\" los valores normales en un rango muy pequeño.       \n",
    "- En todos los demás casos, es preferible la normalización min-max.      \n",
    "\n",
    "El reescalado de características suele ser beneficioso para la mayoría de los algoritmos de aprendizaje. Sin embargo, las implementaciones modernas de los algoritmos de aprendizaje, que se pueden encontrar en bibliotecas populares, son robustas a las características que se encuentran en diferentes rangos.\n",
    "\n",
    "**NOTA**: a los modelos de tipo árbol (DecisionTree, RandomForest, GradientBoosting) les da igual la normalización. No les importa la magnitud exacta de una variable, si no la ordenación de los valores (sólo hacen preguntas < o > qué)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repasamos Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a preparar los datos (X, y) antes de entrenar el modelo y ajustar los hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas que voy a usar en la X\n",
    "columnas_x = [a for a in list(data.columns) if a != \"Survived\"]\n",
    "columnas_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me guardo la variable X con todos los datos \n",
    "X = data[columnas_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable target solo la columna que voy a predecir\n",
    "y = data.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asigno las variables train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.2, random_state = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué es el ajuste de hiperparámetros?\n",
    "Los hiperparámetros son parámetros ajustables que permiten controlar el proceso de entrenamiento de un modelo. Por ejemplo, con redes neuronales, puede decidir el número de capas ocultas y el número de nodos de cada capa. El rendimiento de un modelo depende en gran medida de los hiperparámetros.\n",
    "El ajuste de hiperparámetros, también denominado optimización de hiperparámetros es el proceso de encontrar la configuración de hiperparámetros que produzca el mejor rendimiento. Normalmente, el proceso es manual y costoso desde el punto de vista computacional.\n",
    "\n",
    "Hay diferentes técnicas para elegir este ajuste de hiperparámetros:     \n",
    "    \n",
    "### - Muestreo aleatorio    \n",
    "El muestreo aleatorio admite hiperparámetros discretos y continuos. Admite la terminación anticipada de las series de bajo rendimiento. Algunos usuarios realizan una búsqueda inicial con muestreo aleatorio y luego restringen el espacio de búsqueda para mejorar los resultados.\n",
    "En el muestreo aleatorio, los valores de hiperparámetro se seleccionan aleatoriamente del espacio de búsqueda definido.\n",
    "\n",
    "### - Muestreo de cuadrícula\n",
    "El muestreo de cuadrícula admite hiperparámetros discretos. Use el muestreo de cuadrícula si su presupuesto le permite buscar en el espacio de búsqueda de manera exhaustiva. Admite la terminación anticipada de las series de bajo rendimiento.\n",
    "\n",
    "### - Muestreo bayesiano   \n",
    "El muestreo bayesiano se basa en el algoritmo de optimización bayesiano. Escoge las muestras en función de cómo lo hicieron las anteriores, para que las nuevas muestras mejoren la métrica principal.\n",
    " Para obtener los mejores resultados, se recomienda que el número máximo de series sea mayor o igual que 20 veces el número de hiperparámetros que se está optimizando.\n",
    "El número de series simultáneas afecta a la eficacia del proceso de ajuste. Un menor número de series simultáneas puede provocar una mejor convergencia de muestreo, dado que el menor grado de paralelismo aumenta el número de series que se benefician de las series completadas previamente.\n",
    "\n",
    "Vamos a ver el ajuste de hiperparámetros en cuadrícula con GridSearchCV pero os dejo que investiguéis el muestreo bayesiano con [HyperOpt](https://towardsdatascience.com/hyperopt-hyperparameter-tuning-based-on-bayesian-optimization-7fa32dffaf29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV de sklearn, ¡saludad a vuestro nuevo amigo!\n",
    "Y leed la [documentación](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparámetros tuneables de RandomForest\n",
    "parameters = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducimos para hacer la prueba con diferentes n_estimators\n",
    "params = {\n",
    "     'n_estimators': [400, 600,800]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guaardo en una variable raandom forest\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo el grid search con el algoritmo, los parámetros y verbose paraa que muestre info del proceso\n",
    "grid = GridSearchCV(rfc, params, verbose=1)\n",
    "# entreno el grid con los datos de train\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo los mejores parámetros que me ha dado el modelo\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenaríamos el modelo con los mejores parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si en GridSearchCV() se indica refit=True, tras identificar los mejores hiperparámetros, se reentrena el modelo con ellos y se almacena en .best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenon\n",
    "rfc_params = RandomForestClassifier(n_estimators =  800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc_params.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy\", round(accuracy_score(y_test,y_pred),3))\n",
    "print(\"Precission\",round(precision_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"Recall\", round(recall_score(y_test,y_pred, average = \"weighted\"),3))\n",
    "print(\"F1_score\", round(f1_score(y_test,y_pred,average= \"weighted\"),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar / Exprotar el modelo\n",
    "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(rfc_params, open(\"mi_mejor_modelo\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(\"mi_mejor_modelo\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
